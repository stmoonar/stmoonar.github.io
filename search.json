[{"categories":null,"content":"一级标题 你好，我是XXY，现在在上海极氪实习。bbb\nAa Bb Cc Dd Ee Ff Gg Hh Ii Jj Kk Ll Mm Nn Oo Pp Qq Rr Ss Tt Uu Vv Ww Xx Yy Zz\nAa Bb Cc Dd Ee Ff Gg Hh Ii Jj Kk Ll Mm Nn Oo Pp Qq Rr Ss Tt Uu Vv Ww Xx Yy Zz\n二级标题 hello world. This is so cool!\n对比 hugo hexo 使用语言 go js 响应速度 快 慢 编译速度 hexo是什么\nenp130s0f0np0: flags=4163\u003cUP,BROADCAST,RUNNING,MULTICAST\u003e mtu 1500 inet 192.10.10.109 netmask 255.255.255.0 broadcast 192.10.10.255 inet6 fe80::bace:f6ff:fefe:63a0 prefixlen 64 scopeid 0x20 ether b8:ce:f6:fe:63:a0 txqueuelen 1000 (Ethernet) RX packets 1562313 bytes 131656565 (131.6 MB) RX errors 359073356 dropped 0 overruns 0 frame 359073356 TX packets 16282529 bytes 23251786527 (23.2 GB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0\nenp130s0f1np1: flags=4163\u003cUP,BROADCAST,RUNNING,MULTICAST\u003e mtu 1500 ether b8:ce:f6:fe:63:a1 txqueuelen 1000 (Ethernet) RX packets 56507 bytes 15305497 (15.3 MB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 202361 bytes 34897114 (34.8 MB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0\n2. 隐藏内存分配延迟 The serving framework invokes the step API in every iteration. The latency of step depends on how many new pages need to be mapped in the virtual tensors of KV-cache.\n将计算与内存分配交错：内存需求是可预测的\n你好 二级列表 延迟回收，直接将R1的reqId分配给新来的R2 (此处若R2的contexth小于R1，且在prefill阶段基本上是必然小于的，会不会造成浪费？or这种浪费与重新分配空间的时间相比是否可以忽略？)\n在物理页面需要前就主动分配：最后，只有当vAttention中缓存的物理内存页数低于某个阈值（例如，小于GPU内存的10%）时，我们才会触发内存回收。(上面的问题解决了)\n有序列表 测试一 测试2 测试3 1 2 3 4 from transformers import AutoModel tokenizer = AutoTokenizer.from_pretrained(tf_save_directory) pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True) 三级标题 线性代数中最有用的一些运算符是范数（norm）。 非正式地说，向量的范数是表示一个向量有多大。 这里考虑的大小（size）概念不涉及维度，而是分量的大小。\n在线性代数中，向量范数是将向量映射到标量的函数$f$。性质：\n如果我们按常数因子α缩放向量的所有元素， 其范数也会按相同常数因子的绝对值缩放：$f(\\alpha x) = |\\alpha|f(x)$ 三角不等式：$f(x+y) \\le f(x) + f(y)$ 非负的（只有向量全为0时范数才为0） 范数听起来很像距离的度量。 欧几里得距离和毕达哥拉斯定理中的非负性概念和三角不等式可能会给出一些启发。 事实上，欧几里得距离是一个L2范数： 假设n维向量x中的元素是x1,…,xn，其L2范数是向量元素平方和的平方根\n$$ \\displaystyle{\\|\\mathrm{x}\\|_2 = \\sqrt{\\sum\\limits_{i=0}^n|x_i|^2}} $$ 深度学习中更经常地使用L2范数的平方，也会经常遇到L1范数，它表示为向量元素的绝对值之和：\n$$ \\displaystyle{\\|\\mathrm{x}\\|_1 = \\sum\\limits_{i=1}^n|x_i|} $$ 与L2范数相比，L1范数受异常值的影响较小。\nL2范数计算：\n1 2 x = torch.tensor([3.0. 4.0]) torch.norm(x) L1范数计算：\n1 torch.abs(x).sum() L2范数和L1范数都是更一般的$L_p$范数的特例：\n$$ \\displaystyle{\\|\\mathrm{x}\\|_p = \\bigg( \\sum\\limits_{i=1}^n|x_i|^p \\bigg)^{1/p}} $$ 矩阵X的Frobenius norm是矩阵元素平方和的平方根：\n$$ \\displaystyle{\\|\\mathrm{X}\\|_F = \\sqrt{\\sum\\limits_{i=1}^m\\sum\\limits_{j=1}^nx_{ij}^2}} $$ 也是调用torch.norm()计算。\n在深度学习中，我们经常试图解决优化问题： 最大化分配给观测数据的概率; 最小化预测和真实观测之间的距离。 用向量表示物品（如单词、产品或新闻文章），以便最小化相似项目之间的距离，最大化不同项目之间的距离。 目标，或许是深度学习算法最重要的组成部分（除了数据），通常被表达为范数。\n","description":"","tags":null,"title":"测试","uri":"/post/2024/test/"}]
