[{"categories":null,"content":"了解为什么计算指数加权方差不能正确估计方差。\n作者：Adrian Letchford\t译者：stmoonar\n你很可能熟悉用指数加权法计算平均数的方法。指数加权平均法的原理是对较新的信息赋予较高的权重。指数加权平均法的权重如下：\n$$\\displaystyle{w_t = (1 - \\alpha)^t}$$ 对于$t \\in [0, \\dots, T]$，序列 $X_t$ 的指数加权平均值是这样的：\n$$\\displaystyle{\\bar{X}_T = \\frac{1}{\\sum_t w_t} \\sum_t w_t X_t}$$ 在 Pandas 中，您可以通过以下面的方法轻松计算指数加权移动平均线：\n1 df.ewm(alpha=0.1).mean() 如果你自己计算指数加权平均值，你会发现它与 Pandas 给出的结果一致。但是，我们即将看到，如果我们尝试用方差来计算，我们将得到一个很差的估计值。这就是所谓的估计偏差（estimation bias）。\n什么是偏差（bias）？ 估计量的偏差是指估计量的预期值与被估计参数（本例中为方差）的真实值之间的差值。有偏差的估计值与真实值之差不为零，而无偏差的估计值与真实值之差为零。\n让我们试着测量方差，看看会发生什么。随机变量 $X$ 的方差是：\n$$\\displaystyle{\\sigma^2 = E[(X - \\mu)^2]}$$ 如果我们有 $X$ 的 $n$ 值样本，我们可以尝试用样本的平均值代替期望值 $E[\\cdot]$ 来估计方差：\n$$\\displaystyle{\\frac{1}{n} \\sum_i \\left(X_i - \\mu \\right)^2}$$ 然后用样本的平均值代替 $\\mu$：\n$$\\displaystyle{\\begin{aligned} \\bar{X} \u0026= \\frac{1}{n}\\sum_i X_i \\\\ \\hat{\\sigma}^2 \u0026= \\frac{1}{n} \\sum_i \\left(X_i - \\bar{X} \\right)^2 \\end{aligned}}$$ 我们可以用 Python 写一个简单的模拟，看看我们的估计量得到的 $\\hat{\\sigma}^2$ 有多好。\n1 2 3 4 5 6 7 8 9 10 11 12 13 from scipy.stats import norm num_simulations = 10_000_000 n = 5 # 样本数 # 这样就得到了一个数组，其中每一行都是一个模拟，而列则是样本值。 simulations = norm.rvs(loc=0, scale=1, size=(num_simulations, n)) # 由此得出每个模拟的估计平均值。 avg = simulations.mean(axis=1).reshape(-1, 1) # 使用我们的估计器来估计每个模拟的方差。 estimates = ((simulations - avg)**2).sum(1) / n 译者注：\n代码使用了 norm.rvs 方法从正态分布中随机生成样本值，loc=0 表示正态分布的均值为0，scale=1 表示标准差为1，size=(num_simulations, n) 表示生成的数组将有 num_simulations 行，每行有 n 个样本值。\n现在我们有 1000 万个方差估计值，而真实方差为 1。那么我们的平均估计值是多少呢？计算估计值的平均值：\n1 estimates.mean() 得出约 0.8！ 我们的估计值偏差了 0.2。 这就是偏差！\n偏差从哪里来？ 让我们回到方差的定义以及如何将其转化为样本估计值。为了计算我们的估计值，我们将 $\\mu$ 换成了样本平均值：\n$$\\displaystyle{\\frac{1}{n} \\sum_i \\left(X_i - \\mu \\right)^2 \\quad\\Rightarrow\\quad \\frac{1}{n} \\sum_i \\left(X_i - \\bar{X} \\right)^2}$$ 这就是偏差产生的原因。小样本的平均值（$\\bar{X}$）会比总体的平均值（$\\mu$）更接近这些样本，通过下面的例子可以更加直观。\n图 1 显示了一个有 100 个随机点的例子，这些点的中心点（即平均值）为 0，其中有 5 个点被随机选中，它们的平均值用黑叉表示。这 5 个样本的平均值就是最接近这 5 个样本的点。根据定义，样本的平均数会比总体平均数更接近样本。因此：\n$$\\displaystyle{\\frac{1}{n} \\sum_i \\left(X_i - \\bar{X} \\right)^2 \\quad\\lt\\quad \\frac{1}{n} \\sum_i \\left(X_i - \\mu \\right)^2}$$ 我们的估计值会低于（underestimate ） $X$ 的方差！\n图 1：偏差示意图。 这是一幅平均值为（0，0）的 100 个随机点的图。图中随机选取了 5 个点并突出显示。黑叉表示这 5 个随机点的平均值。 事实上，如果重复一次上面的 Python 模拟，但是使用 0（总体平均值）代替样本平均值：\n1 avg = 0 那么得到的样本方差的平均值就是 1：\n1 estimates.mean() 如果知道总体均值，我们就可以从一组样本中得到方差的无偏估计值。但实际上，我们并不知道总体均值。幸运的是，我们可以量化偏差并加以纠正。\n量化偏差 到目前为止，我们已经看到 $\\hat{\\sigma}^2$ 是对总体方差的一个有偏差的估计。我们通过模拟多个样本得到 $\\hat{\\sigma}^2$ 并取平均值发现了这一点。模拟结果表明：\n$$\\displaystyle{E[\\hat{\\sigma}^2] \\ne \\sigma^2}$$ 我们现在要摆脱模拟，计算 $E[\\hat{\\sigma}^2]$ 的精确值。我们可以通过展开式（原文：expanding it out）来实现。我们从：\n$$\\displaystyle{E[\\hat{\\sigma}^2] = E \\left[ \\frac{1}{n} \\sum_i \\left(X_i - \\bar{X} \\right)^2 \\right]}$$ 开始。\n我们可以让$\\bar{X} = \\mu - (\\mu - \\bar{X})$，这意味着我们可以展开为\n$$\\displaystyle{E[\\hat{\\sigma}^2] = E \\left[ \\frac{1}{n} \\sum_i \\left((X_i - \\mu)- (\\bar{X} - \\mu) \\right)^2 \\right]}$$ 通过一些代数知识，我们可以展开平方式：\n$$\\displaystyle{\\begin{aligned} E[\\hat{\\sigma}^2] \u0026= E \\left[ \\frac{1}{n} \\sum_i \\left((X_i - \\mu)^2 - 2(\\bar{X} - \\mu)(X_i - \\mu) + (\\bar{X} - \\mu)^2\\right) \\right] \\\\ \u0026= E \\left[ \\frac{1}{n} \\sum_i (X_i - \\mu)^2 - 2(\\bar{X} - \\mu) \\frac{1}{n} \\sum_i(X_i - \\mu) + \\frac{1}{n} \\sum_i(\\bar{X} - \\mu)^2 \\right] \\\\ \u0026= E \\left[ \\frac{1}{n} \\sum_i (X_i -\\mu)^2 - 2(\\bar{X} - \\mu) \\frac{1}{n} \\sum_i(X_i - \\mu) + (\\bar{X} -\\mu)^2 \\right] \\end{aligned}}$$ 现在，请注意：\n$$\\displaystyle{\\frac{1}{n} \\sum_i(X_i - \\mu) = \\frac{1}{n} \\sum_i X_i - \\frac{1}{n} \\sum_i \\mu = \\frac{1}{n} \\sum_i X_i - \\mu = \\bar{X} - \\mu}$$ 这意味着：\n$$\\displaystyle{\\begin{aligned} E[\\hat{\\sigma}^2] \u0026= E \\left[ \\frac{1}{n} \\sum_i (X_i - \\mu)^2 - 2(\\bar{X} - \\mu)^2 + (\\bar{X} - \\mu)^2 \\right] \\\\ \u0026= E \\left[ \\frac{1}{n} \\sum_i (X_i - \\mu)^2 - (\\bar{X} - \\mu)^2 \\right] \\\\ \u0026= E \\left[ \\frac{1}{n} \\sum_i (X_i - \\mu)^2 \\right] - E \\left[ (\\bar{X} - \\mu)^2 \\right] \\end{aligned}}$$ 这里的妙处是：\n$$\\displaystyle{E \\left[ \\frac{1}{n} \\sum_i (X_i - \\mu)^2 \\right] = \\sigma^2}$$ 意味着：\n$$\\displaystyle{\\begin{aligned} E[\\hat{\\sigma}^2] \u0026= \\sigma^2 - E \\left[ (\\bar{X} - \\mu)^2 \\right] \\end{aligned}}$$ 其中 $E \\left[ (\\bar{X} - \\mu)^2 \\right]$ 是样本平均数的方差。 根据Bienaymé’s identity ，我们知道它等于\n$$\\displaystyle{E \\left[ (\\bar{X} - \\mu)^2 \\right] = \\frac{1}{n}\\sigma^2}$$ 这让我们得到：\n$$\\displaystyle{E[\\hat{\\sigma}^2] = (1 - \\frac{1}{n}) \\sigma^2 = (1 - \\frac{1}{5}) \\times 1 = 0.8}$$ 回想一下我们的 Python 模拟；样本数为 $n=5$，真实方差为 $\\sigma^2 = 1$，估计方差为 $\\hat{\\sigma}^2 = 0.8$。 如果我们将 $n$ 和 $\\sigma^2$ 插入上述公式，就会得到有偏差的答案：\n$$\\displaystyle{E[\\hat{\\sigma}^2] = (1 - \\frac{1}{n}) \\sigma^2 = (1 - \\frac{1}{5}) \\times 1 = 0.8}$$ 无偏估计量 现在我们知道了 $E[\\hat{\\sigma}^2]$ 的精确值，就可以想办法修正 $\\hat{\\sigma}^2$，使其成为 $\\sigma^2$ 的无偏估计值。\n修正项为：\n$$\\displaystyle{\\frac{n}{n-1}}$$ 通过演算，我们可以看到：\n$$\\displaystyle{\\begin{aligned} E[\\frac{n}{n-1} \\hat{\\sigma}^2] \u0026=\\frac{n}{n-1} E[\\hat{\\sigma}^2] \\\\ \u0026= \\frac{n}{n-1}(1 - \\frac{1}{n}) \\sigma^2 \\\\ \u0026= \\frac{n(1 - \\frac{1}{n})}{n-1} \\sigma^2 \\\\ \u0026= \\frac{n - 1}{n-1} \\sigma^2 \\\\ \u0026= \\sigma^2 \\end{aligned}}$$ 因此，从一组样本中得到的 $\\sigma^2$ 的无偏估计值是：\n$$\\displaystyle{\\begin{aligned} \\frac{n}{n-1} \\hat{\\sigma}^2 \u0026= \\frac{n}{n-1} \\frac{1}{n} \\sum_i \\left(X_i - \\bar{X} \\right)^2 \\\\ \u0026= \\frac{1}{n-1} \\sum_i \\left(X_i - \\bar{X} \\right)^2 \\end{aligned}}$$ 无偏加权估计量 现在，我们将上述内容扩展到样本加权的情况。 加权样本平均值为：\n$$\\displaystyle{\\bar{X} = \\frac{1}{\\sum_i w_i} \\sum_i w_i X_i}$$ 加权方差为：\n$$\\hat{\\sigma}^2 = \\frac{1}{\\sum_i w_i} \\sum_i w_i\\left(X_i - \\bar{X} \\right)^2$$ 按照与之前完全相同的展开过程，我们得出：\n$$\\displaystyle{\\begin{aligned} E[\\hat{\\sigma}^2] \u0026= \\sigma^2 - E \\left[ (\\bar{X} - \\mu)^2 \\right] \\end{aligned}}$$ 平均值的方差为：\n$$\\displaystyle{\\begin{aligned} E \\left[ (\\bar{X} - \\mu)^2 \\right] \u0026= \\text{var}(\\bar{X}) \\\\ \u0026= \\text{var}\\left(\\frac{1}{\\sum w_i} \\sum w_i X_i \\right) \\\\ \u0026= \\frac{1}{(\\sum w_i)^2} \\sum \\text{var} (w_i X_i) \\\\ \u0026= \\frac{1}{(\\sum w_i)^2} \\sum w_i^2 \\text{var} (X_i) \\\\ \u0026= \\frac{\\sum w_i^2}{(\\sum w_i)^2} \\sigma^2 \\end{aligned}}$$ $var$是计算方差。\n这样我们就得到：\n$$\\displaystyle{\\begin{aligned} E[\\hat{\\sigma}^2] \u0026= \\sigma^2 - \\frac{\\sum w_i^2}{(\\sum w_i)^2} \\\\ \\sigma^2 \u0026= \\left(1 - \\frac{\\sum w_i^2}{(\\sum w_i)^2} \\right)\\sigma^2 \\end{aligned}}$$ 偏差修正项为：\n$$\\displaystyle{b = \\frac{(\\sum w_i)^2}{(\\sum w_i)^2 - \\sum w_i^2}}$$ 这意味着方差的无偏加权估计值为：\n$$\\displaystyle{b \\hat{\\sigma}^2}$$ 复现 Pandas 指数加权方差 现在，我们拥有了复现 Pandas 指数加权方差所需的所有工具。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 import numpy as np import pandas as pd N = 1000 # 创建一些fake data df = pd.DataFrame() df['data'] = np.random.randn(N) # 为 EWM 设置半衰期，并将其转换为 alpha 值进行计算。 halflife = 10 a = 1 - np.exp(-np.log(2)/halflife) # alpha # 这是 Pandas 的 ewm df['var_pandas'] = df.ewm(alpha=a).var() # 初始化变量 varcalc = np.zeros(len(df)) # 计算指数移动方差 for i in range(0, N): x = df['data'].iloc[0:i+1].values # Weights n = len(x) w = (1-a)**np.arange(n-1, -1, -1) # 顺序相反，以便与序列顺序一致 # 计算指数移动平均线 ewma = np.sum(w * x) / np.sum(w) # 计算偏差修正项 bias = np.sum(w)**2 / (np.sum(w)**2 - np.sum(w**2)) # 计算带偏差修正的指数移动方差 varcalc[i] = bias * np.sum(w * (x - ewma)**2) / np.sum(w) df['var_calc'] = varcalc 这样我们就得到了一个下面这样的 DataFrame：\n可以看到估计值和 Pandas 计算的值是相同的。\n","description":"","tags":null,"title":"复现Pandas指数加权方差","uri":"/post/2024/replicating-pandas-exponentially-weighted-variance/"},{"categories":null,"content":"一级标题 你好，我是XXY，现在在上海极氪实习。bbb hhh\nAa Bb Cc Dd Ee Ff Gg Hh Ii Jj Kk Ll Mm Nn Oo Pp Qq Rr Ss Tt Uu Vv Ww Xx Yy Zz\nAa Bb Cc Dd Ee Ff Gg Hh Ii Jj Kk Ll Mm Nn Oo Pp Qq Rr Ss Tt Uu Vv Ww Xx Yy Zz\n二级标题 hello world. This is so cool!\n对比 hugo hexo 使用语言 go js 响应速度 快 慢 编译速度 hexo是什么\nenp130s0f0np0: flags=4163\u003cUP,BROADCAST,RUNNING,MULTICAST\u003e mtu 1500 inet 192.10.10.109 netmask 255.255.255.0 broadcast 192.10.10.255 inet6 fe80::bace:f6ff:fefe:63a0 prefixlen 64 scopeid 0x20 ether b8:ce:f6:fe:63:a0 txqueuelen 1000 (Ethernet) RX packets 1562313 bytes 131656565 (131.6 MB) RX errors 359073356 dropped 0 overruns 0 frame 359073356 TX packets 16282529 bytes 23251786527 (23.2 GB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0\nenp130s0f1np1: flags=4163\u003cUP,BROADCAST,RUNNING,MULTICAST\u003e mtu 1500 ether b8:ce:f6:fe:63:a1 txqueuelen 1000 (Ethernet) RX packets 56507 bytes 15305497 (15.3 MB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 202361 bytes 34897114 (34.8 MB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0\n2. 隐藏内存分配延迟 The serving framework invokes the step API in every iteration. The latency of step depends on how many new pages need to be mapped in the virtual tensors of KV-cache.\n将计算与内存分配交错：内存需求是可预测的\n你好 二级列表 延迟回收，直接将R1的reqId分配给新来的R2 (此处若R2的contexth小于R1，且在prefill阶段基本上是必然小于的，会不会造成浪费？or这种浪费与重新分配空间的时间相比是否可以忽略？)\n在物理页面需要前就主动分配：最后，只有当vAttention中缓存的物理内存页数低于某个阈值（例如，小于GPU内存的10%）时，我们才会触发内存回收。(上面的问题解决了)\n有序列表 测试一 测试2 测试3 1 2 3 4 from transformers import AutoModel tokenizer = AutoTokenizer.from_pretrained(tf_save_directory) pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True) 三级标题 线性代数中最有用的一些运算符是范数（norm）。 非正式地说，向量的范数是表示一个向量有多大。 这里考虑的大小（size）概念不涉及维度，而是分量的大小。\n在线性代数中，向量范数是将向量映射到标量的函数$f$。性质：\n如果我们按常数因子α缩放向量的所有元素， 其范数也会按相同常数因子的绝对值缩放：$f(\\alpha x) = |\\alpha|f(x)$ 三角不等式：$f(x+y) \\le f(x) + f(y)$ 非负的（只有向量全为0时范数才为0） 范数听起来很像距离的度量。 欧几里得距离和毕达哥拉斯定理中的非负性概念和三角不等式可能会给出一些启发。 事实上，欧几里得距离是一个L2范数： 假设n维向量x中的元素是x1,…,xn，其L2范数是向量元素平方和的平方根\n$$ \\displaystyle{\\|\\mathrm{x}\\|_2 = \\sqrt{\\sum\\limits_{i=0}^n|x_i|^2}} $$ 深度学习中更经常地使用L2范数的平方，也会经常遇到L1范数，它表示为向量元素的绝对值之和：\n$$ \\displaystyle{\\|\\mathrm{x}\\|_1 = \\sum\\limits_{i=1}^n|x_i|} $$ 与L2范数相比，L1范数受异常值的影响较小。\nL2范数计算：\n1 2 x = torch.tensor([3.0. 4.0]) torch.norm(x) L1范数计算：\n1 torch.abs(x).sum() L2范数和L1范数都是更一般的$L_p$范数的特例：\n$$ \\displaystyle{\\|\\mathrm{x}\\|_p = \\bigg( \\sum\\limits_{i=1}^n|x_i|^p \\bigg)^{1/p}} $$ 矩阵X的Frobenius norm是矩阵元素平方和的平方根：\n$$ \\displaystyle{\\|\\mathrm{X}\\|_F = \\sqrt{\\sum\\limits_{i=1}^m\\sum\\limits_{j=1}^nx_{ij}^2}} $$ 也是调用torch.norm()计算。\n在深度学习中，我们经常试图解决优化问题： 最大化分配给观测数据的概率; 最小化预测和真实观测之间的距离。 用向量表示物品（如单词、产品或新闻文章），以便最小化相似项目之间的距离，最大化不同项目之间的距离。 目标，或许是深度学习算法最重要的组成部分（除了数据），通常被表达为范数。\n","description":"","tags":null,"title":"测试","uri":"/post/2024/test/"}]
