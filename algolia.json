[{"categories":"post","content":"\u003ch1 id=\"一级标题\"\u003e一级标题\u003c/h1\u003e\n\u003cp\u003e你好，我是\u003cstrong\u003eXXY\u003c/strong\u003e，现在在上海\u003cmark\u003e极氪\u003c/mark\u003e实习。\u003cem\u003ebbb\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eAa Bb Cc Dd Ee Ff Gg Hh Ii Jj Kk Ll Mm Nn Oo Pp Qq Rr Ss Tt Uu Vv Ww Xx Yy Zz\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eAa Bb Cc Dd Ee Ff Gg Hh Ii Jj Kk Ll Mm Nn Oo Pp Qq Rr Ss Tt Uu Vv Ww Xx Yy Zz\u003c/em\u003e\u003c/p\u003e\n\u003cimg src=\"https://pics.stmoonar.me/mypics/typora/2024/08/04/202408040031084.jpg\" alt=\"104862001_p1\" style=\"zoom:50%;\" /\u003e\n\u003ch2 id=\"二级标题\"\u003e二级标题\u003c/h2\u003e\n\u003cp\u003ehello world. This is so cool!\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e对比\u003c/th\u003e\n\u003cth\u003ehugo\u003c/th\u003e\n\u003cth\u003ehexo\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e使用语言\u003c/td\u003e\n\u003ctd\u003ego\u003c/td\u003e\n\u003ctd\u003ejs\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e响应速度\u003c/td\u003e\n\u003ctd\u003e快\u003c/td\u003e\n\u003ctd\u003e慢\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e编译速度\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003ehexo是什么\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eenp130s0f0np0: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt;  mtu 1500\ninet 192.10.10.109  netmask 255.255.255.0  broadcast 192.10.10.255\ninet6 fe80::bace:f6ff:fefe:63a0  prefixlen 64  scopeid 0x20\u003clink\u003e\nether b8:ce:f6:fe:63:a0  txqueuelen 1000  (Ethernet)\nRX packets 1562313  bytes 131656565 (131.6 MB)\nRX errors 359073356  dropped 0  overruns 0  frame 359073356\nTX packets 16282529  bytes 23251786527 (23.2 GB)\nTX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\u003c/p\u003e\n\u003cp\u003eenp130s0f1np1: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt;  mtu 1500\nether b8:ce:f6:fe:63:a1  txqueuelen 1000  (Ethernet)\nRX packets 56507  bytes 15305497 (15.3 MB)\nRX errors 0  dropped 0  overruns 0  frame 0\nTX packets 202361  bytes 34897114 (34.8 MB)\nTX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch4 id=\"2-隐藏内存分配延迟\"\u003e2. 隐藏内存分配延迟\u003c/h4\u003e\n\u003cp\u003eThe serving framework invokes the \u003ccode\u003estep\u003c/code\u003e API in every iteration. The latency of \u003ccode\u003estep\u003c/code\u003e depends on \u003cstrong\u003ehow many new pages\u003c/strong\u003e need to be mapped in the virtual tensors of KV-cache.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e将计算与内存分配交错：内存需求是可预测的\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e你好\u003c/li\u003e\n\u003cli\u003e二级列表\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e延迟回收，直接将R1的reqId分配给新来的R2 \u003cstrong\u003e(此处若R2的\u003cem\u003econtexth\u003c/em\u003e小于R1，且在prefill阶段基本上是必然小于的，会不会造成浪费？or这种浪费与重新分配空间的时间相比是否可以忽略？)\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e在物理页面需要前就主动分配：最后，只有当vAttention中缓存的物理内存页数低于某个阈值（例如，小于GPU内存的10%）时，我们才会触发内存回收。\u003cstrong\u003e(上面的问题解决了)\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"有序列表\"\u003e有序列表\u003c/h4\u003e\n\u003col\u003e\n\u003cli\u003e测试一\u003c/li\u003e\n\u003cli\u003e测试2\u003c/li\u003e\n\u003cli\u003e测试3\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e\n\u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e1\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e2\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e3\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e4\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003etransformers\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eAutoModel\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003etokenizer\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eAutoTokenizer\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003efrom_pretrained\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003etf_save_directory\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003ept_model\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eAutoModelForSequenceClassification\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003efrom_pretrained\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003etf_save_directory\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003efrom_tf\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"kc\"\u003eTrue\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003ch3 id=\"三级标题\"\u003e三级标题\u003c/h3\u003e\n\u003cp\u003e线性代数中最有用的一些运算符是\u003cem\u003e范数\u003c/em\u003e（norm）。 非正式地说，向量的\u003cem\u003e范数\u003c/em\u003e是表示一个\u003cstrong\u003e向量有多大\u003c/strong\u003e。 这里考虑的\u003cem\u003e大小\u003c/em\u003e（size）概念不涉及维度，而是\u003cstrong\u003e分量的大小\u003c/strong\u003e。\u003c/p\u003e\n\u003cp\u003e在线性代数中，向量范数是将向量映射到标量的函数$f$。性质：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e如果我们按常数因子α缩放向量的所有元素， 其范数也会按相同常数因子的\u003cem\u003e绝对值\u003c/em\u003e缩放：$f(\\alpha x) = |\\alpha|f(x)$\u003c/li\u003e\n\u003cli\u003e三角不等式：$f(x+y) \\le f(x) + f(y)$\u003c/li\u003e\n\u003cli\u003e非负的（只有向量全为0时范数才为0）\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e范数听起来很像距离的度量。 欧几里得距离和毕达哥拉斯定理中的非负性概念和三角不等式可能会给出一些启发。 事实上，欧几里得距离是一个L2范数： 假设n维向量x中的元素是x1,…,xn，其L2\u003cem\u003e范数\u003c/em\u003e是向量元素平方和的平方根\u003c/p\u003e\n\u003cdiv\u003e\n$$\n\\displaystyle{\\|\\mathrm{x}\\|_2 = \\sqrt{\\sum\\limits_{i=0}^n|x_i|^2}}\n$$\n\u003c/div\u003e\n\u003cp\u003e深度学习中更经常地使用\u003cstrong\u003eL2范数的平方\u003c/strong\u003e，也会经常遇到L1范数，它表示为向量元素的绝对值之和：\u003c/p\u003e\n\u003cdiv\u003e\n$$\n\\displaystyle{\\|\\mathrm{x}\\|_1 = \\sum\\limits_{i=1}^n|x_i|}\n$$\n\u003c/div\u003e\n\u003cp\u003e与L2范数相比，L1范数受异常值的影响较小。\u003c/p\u003e\n\u003cp\u003eL2范数计算：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e\n\u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e1\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e2\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etorch\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003etensor\u003c/span\u003e\u003cspan class=\"p\"\u003e([\u003c/span\u003e\u003cspan class=\"mf\"\u003e3.0\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e \u003cspan class=\"mf\"\u003e4.0\u003c/span\u003e\u003cspan class=\"p\"\u003e])\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003etorch\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003enorm\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003eL1范数计算：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e\n\u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e1\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003etorch\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eabs\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003esum\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003eL2范数和L1范数都是更一般的$L_p$范数的特例：\u003c/p\u003e\n\u003cdiv\u003e\n$$\n\\displaystyle{\\|\\mathrm{x}\\|_p = \\bigg( \\sum\\limits_{i=1}^n|x_i|^p \\bigg)^{1/p}}\n$$\n\u003c/div\u003e\n\u003cp\u003e矩阵X的\u003cem\u003eFrobenius norm\u003c/em\u003e是矩阵元素平方和的平方根：\u003c/p\u003e\n\u003cdiv\u003e\n$$\n\\displaystyle{\\|\\mathrm{X}\\|_F = \\sqrt{\\sum\\limits_{i=1}^m\\sum\\limits_{j=1}^nx_{ij}^2}}\n$$\n\u003c/div\u003e\n\u003cp\u003e也是调用\u003ccode\u003etorch.norm()\u003c/code\u003e计算。\u003c/p\u003e\n\u003cp\u003e在深度学习中，我们经常试图解决优化问题： \u003cem\u003e最大化\u003c/em\u003e分配给观测数据的概率; \u003cem\u003e最小化\u003c/em\u003e预测和真实观测之间的距离。 用向量表示物品（如单词、产品或新闻文章），以便最小化相似项目之间的距离，最大化不同项目之间的距离。 \u003cstrong\u003e目标\u003c/strong\u003e，或许是深度学习算法最重要的组成部分（除了数据），\u003cstrong\u003e通常被表达为范数\u003c/strong\u003e。\u003c/p\u003e\n","date":1722702017,"description":"","fuzzywordcount":1200,"kind":"page","lang":"zh-cn","lastmod":1725954715,"objectID":"02b94ec98d0e7d9813e081dd062ddb80","publishdate":1722702017,"relpermalink":"/post/2024/%E6%B5%8B%E8%AF%95/","section":"post","summary":"一级标题 你好，我是XXY，现在在上海极氪实习。bbb Aa Bb Cc Dd Ee Ff Gg Hh Ii Jj Kk Ll Mm Nn Oo Pp Qq Rr Ss Tt Uu Vv Ww Xx Yy Zz Aa Bb Cc Dd Ee Ff Gg Hh Ii Jj Kk Ll Mm Nn Oo Pp Qq Rr Ss Tt Uu Vv Ww Xx Yy Zz 二级标题 hello world. This is so cool!","tags":null,"title":"测试","url":"http://localhost:1313/post/2024/%E6%B5%8B%E8%AF%95/","wordcount":1112}]